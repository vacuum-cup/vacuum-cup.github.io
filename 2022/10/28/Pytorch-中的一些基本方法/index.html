<!DOCTYPE html>
<html lang="zh">
    <!-- title -->


    

<!-- keywords -->



<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="xnf">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="xnf">
    
        <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    
    <meta name="description" content="">
    <meta name="description" content="1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法 7. torch.split方法 8. tensor.expand() 方法">
<meta property="og:type" content="article">
<meta property="og:title" content="(Pytorch)中的一些基本方法">
<meta property="og:url" content="https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/28/Pytorch-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法 7. torch.split方法 8. tensor.expand() 方法">
<meta property="og:locale">
<meta property="og:image" content="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028112608381.png">
<meta property="og:image" content="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028112735293.png">
<meta property="og:image" content="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028145413973.png">
<meta property="article:published_time" content="2022-10-28T03:18:51.000Z">
<meta property="article:modified_time" content="2023-02-13T02:16:08.264Z">
<meta property="article:author" content="xnf">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028112608381.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/assets/favicon.ico">
    
    <title>(Pytorch)中的一些基本方法 · dudu&#39;s blog</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link rel="preload" href="/css/style.css?v=20211217" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="/css/dark.css?v=20211217" as="style">
    <link rel="stylesheet" href="/css/dark.css">
    <link rel="stylesheet" href="/css/mobile.css?v=20211217" media="(max-width: 960px)">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js?v=20211217" as="script">
    <link rel="preload" href="/scripts/dark.js?v=20211217" as="script">
    <link rel="preload" href="/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->
    
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<meta name="generator" content="Hexo 5.4.2"></head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js" />')
        }
    </script>
    
        <body class="post-body">
    
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        
            <div class="header-sidebar-menu">
        
            
                <div style="padding-left: 1px;">&#xe775;</div>
            
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href=/>Dudu's Blog.</a>
        </span>
    </div>
    <!-- toggle banner for post layout -->
    
        
            <div class="banner">
        
            <div class="blog-title header-element">
                <a href="/">Dudu&#39;s Blog.</a>
            </div>
            <div class="post-title header-element">
                <a href="#" class="post-name">(Pytorch)中的一些基本方法</a>
            </div>
        </div>
    
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- back to top button -->
    <div class="footer-fixed-element">
        
            <div class="back-top back-top-hidden">
        
        
            <div>&#xe639;</div>
        
        </div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="







    height:50vh;

">
    
    <!-- 主页  -->
    
        
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
                (Pytorch)中的一些基本方法
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
                
            <!-- 404 -->
            
        </p>
        <!-- 文章页 meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
    
</div>

                
                
                    <div class="post-intro-read">
                        <span>字数统计: <span class="post-count word-count">1.5k</span>阅读时长: <span class="post-count reading-time">7 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2022/10/28</span>
                    <!-- busuanzi -->
                    
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <!-- toc -->
<ul>
<li><a href="#1-torchchunk方法">1.
<strong>torch.chunk方法</strong></a></li>
<li><a href="#2-torchcumsum方法">2.
<strong>torch.cumsum方法</strong></a></li>
<li><a href="#3-torchcat方法">3. <strong>torch.cat方法</strong></a></li>
<li><a href="#4-torchstack方法"><strong>4.
torch.stack方法</strong></a></li>
<li><a href="#5-torchmax方法"><strong>5. torch.max方法</strong></a></li>
<li><a href="#6-squeeze与unsqueezef方法">6.
<strong>squeeze与unsqueezef方法</strong></a></li>
<li><a href="#7-torchsplit方法">7. torch.split方法</a></li>
<li><a href="#8-tensorexpand-方法">8. tensor.expand() 方法</a></li>
</ul>
<!-- tocstop -->
<span id="more"></span>
<h2><span id="1-torchchunk方法">1. <strong>torch.chunk方法</strong></span></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor.chunk(chunks,dim=<span class="number">0</span>) <span class="comment"># 可以参考torch.chunk()</span></span><br></pre></td></tr></table></figure>
<p><code>torch.chunk</code></p>
<p><img src="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028112608381.png"></p>
<ul>
<li>tensor (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>)
– the tensor to split</li>
<li>chunks (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)
- number of chunks to return（分割的块数）</li>
<li>dim (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)
- dimension along which to split the tensor（沿着哪个轴分块）</li>
</ul>
<p><strong>实例：</strong></p>
<figure>
<img src="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028112735293.png" alt="实例">
<figcaption aria-hidden="true">实例</figcaption>
</figure>
<h2><span id="2-torchcumsum方法">2. <strong>torch.cumsum方法</strong></span></h2>
<p>返回 输入张量指定维度累加和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cumsum(<span class="built_in">input</span>, dim, *, dtype=<span class="literal">None</span>, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure>
<p>参数：</p>
<ul>
<li>input (Tensor) 输入张量</li>
<li>dim (int) 操作的维度</li>
</ul>
<p>计算原理:</p>
<figure>
<img src="https://wgx--img.oss-cn-qingdao.aliyuncs.com/img/image-20221028145413973.png" alt="cumsum 计算原理">
<figcaption aria-hidden="true">cumsum 计算原理</figcaption>
</figure>
<h2><span id="3-torchcat方法">3. <strong>torch.cat方法</strong></span></h2>
<p>在给定维数中连接给定序列的 seq
张量。所有张量必须具有相同的形状(连接维数除外)或为空。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure>
<p>实例：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[ <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>]])</span><br><span class="line">&gt;&gt;&gt; torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>],</span><br><span class="line">        [ <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>],</span><br><span class="line">        [ <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>]])</span><br><span class="line">&gt;&gt;&gt; torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>,  <span class="number">0</span>.<span class="number">6580</span>, -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>,  <span class="number">0</span>.<span class="number">6580</span>,</span><br><span class="line">         -<span class="number">1</span>.<span class="number">0969</span>, -<span class="number">0</span>.<span class="number">4614</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>, -<span class="number">0</span>.<span class="number">1034</span>, -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>, -<span class="number">0</span>.<span class="number">1034</span>,</span><br><span class="line">         -<span class="number">0</span>.<span class="number">5790</span>,  <span class="number">0</span>.<span class="number">1497</span>]])</span><br></pre></td></tr></table></figure>
<h2><span id="4-torchstack方法"><strong>4. torch.stack方法</strong></span></h2>
<p>沿着一个新的维数连接一系列张量。</p>
<p>所有张量的大小必须相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(tensors, dim=<span class="number">0</span>, *, out=<span class="literal">None</span>) → Tensor    <span class="comment"># 插入了一个维度</span></span><br></pre></td></tr></table></figure>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">l = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">tag = torch.FloatTensor(l, l, <span class="number">2</span>).fill_(<span class="number">0</span>)</span><br><span class="line">tag1 = torch.FloatTensor(l, l, <span class="number">2</span>).fill_(<span class="number">5</span>)</span><br><span class="line">tag2 = torch.FloatTensor(l, l, <span class="number">2</span>).fill_(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">tags = torch.stack([tag,tag1,tag2],dim=<span class="number">0</span>)</span><br><span class="line">display(tags.shape)</span><br><span class="line">tags = torch.stack([tag,tag1,tag2],dim=<span class="number">1</span>)</span><br><span class="line">tags.shape</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<h2><span id="5-torchmax方法"><strong>5. torch.max方法</strong></span></h2>
<p><strong>方式一：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>) → Tensor</span><br></pre></td></tr></table></figure>
<p>返回所有元素的最大值</p>
<p>实例：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[ <span class="number">0</span>.<span class="number">6763</span>,  <span class="number">0</span>.<span class="number">7445</span>, -<span class="number">2</span>.<span class="number">2369</span>]])</span><br><span class="line">&gt;&gt;&gt; torch.max(a)</span><br><span class="line">tensor(<span class="number">0</span>.<span class="number">7445</span>)</span><br></pre></td></tr></table></figure>
<p><strong>方式二：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>, *, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>返回一个命名元组(values,
indexes)，其中values是给定维度dim中输入张量的每一行的最大值。而indexes是找到的每个最大值的索引位置(argmax)。</p>
<p>实例：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[-<span class="number">1</span>.<span class="number">2360</span>, -<span class="number">0</span>.<span class="number">2942</span>, -<span class="number">0</span>.<span class="number">1222</span>,  <span class="number">0</span>.<span class="number">8475</span>],</span><br><span class="line">        [ <span class="number">1</span>.<span class="number">1949</span>, -<span class="number">1</span>.<span class="number">1127</span>, -<span class="number">2</span>.<span class="number">2379</span>, -<span class="number">0</span>.<span class="number">6702</span>],</span><br><span class="line">        [ <span class="number">1</span>.<span class="number">5717</span>, -<span class="number">0</span>.<span class="number">9207</span>,  <span class="number">0</span>.<span class="number">1297</span>, -<span class="number">1</span>.<span class="number">8768</span>],</span><br><span class="line">        [-<span class="number">0</span>.<span class="number">6172</span>,  <span class="number">1</span>.<span class="number">0036</span>, -<span class="number">0</span>.<span class="number">6060</span>, -<span class="number">0</span>.<span class="number">2432</span>]])</span><br><span class="line">&gt;&gt;&gt; torch.max(a, <span class="number">1</span>)</span><br><span class="line">torch.return_types.max(values=tensor([<span class="number">0</span>.<span class="number">8475</span>, <span class="number">1</span>.<span class="number">1949</span>, <span class="number">1</span>.<span class="number">5717</span>, <span class="number">1</span>.<span class="number">0036</span>]), indices=tensor([<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p><strong>方式二，个人理解：</strong></p>
<p>​ 给定一个张量 t 和维度d，设 t 的维度为 <code>x*y*z</code></p>
<p>最后的返回值values（仅表示思路 ）:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d = <span class="number">0</span> <span class="comment"># 举例</span></span><br><span class="line">output = torch.zeros(y,z)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(z):</span><br><span class="line">        output[i][j] = <span class="built_in">max</span>(t[:][i][j])</span><br></pre></td></tr></table></figure>
<h2><span id="6squeeze与unsqueezef方法">6.
<strong>squeeze与unsqueezef方法</strong></span></h2>
<p><strong>给tensor删除或者添加维度为1的维度</strong></p>
<ul>
<li><p>squeeze() 方法</p>
<blockquote>
<p>在<a href="https://so.csdn.net/so/search?q=pytorch&amp;spm=1001.2101.3001.7020">pytorch</a>中，用torch.squeeze()函数或者tensor的自身成员函数squeeze()去除维度为1的维度。</p>
<p>使用示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">x_ = x.squeeze() <span class="comment"># 默认删除所大小为1 的维度</span></span><br><span class="line"><span class="built_in">print</span>(x_.shape)</span><br><span class="line">y = torch.squeeze(x,dim=<span class="number">1</span>)  <span class="comment"># 指定维度</span></span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line">z = torch.squeeze(x,dim=<span class="number">2</span>)  <span class="comment"># dim=2 维度不是1  无法去除 但不会报错</span></span><br><span class="line"><span class="built_in">print</span>(z.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
</blockquote></li>
<li><p>unsqueeze() 方法</p>
<blockquote>
<p>在pytorch中，用自带的torch.unsqueeze()和tensor的<a href="https://so.csdn.net/so/search?q=成员函数&amp;spm=1001.2101.3001.7020">成员函数</a>unsqueeze()可以为tensor添加维度为1的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">e = torch.unsqueeze(x, dim=<span class="number">0</span>)  <span class="comment"># 在第一维度添加维度</span></span><br><span class="line"><span class="built_in">print</span>(e.shape)</span><br><span class="line">f = x.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(f.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
</blockquote></li>
</ul>
<h2><span id="7-torchsplit方法">7. torch.split方法</span></h2>
<blockquote>
<p>按块大小拆分张量</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.split(tensor, split_size_or_sections, dim = <span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>tensor 为待拆分张量</p></li>
<li><p>dim 指定张量拆分的所在维度，即在第几维对张量进行拆分</p></li>
<li><p>split_size_or_sections 表示在 dim
维度拆分张量时每一块在该维度的尺寸大小 (int)，或各块尺寸大小的列表
(list)</p></li>
<li><p>指定每一块的尺寸大小后，如果在该维度无法整除，则最后一块会取余数，尺寸较小一些</p>
<blockquote>
<p>如：长度为 10 的张量，按单位长度 3 拆分，则前三块长度为
3，最后一块长度为 1 函数返回：所有拆分后的张量所组成的 tuple
函数并不会改变原 tensor</p>
</blockquote></li>
</ul>
<p><strong>实例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">10</span>).reshape(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line">b,c = torch.split(a,<span class="number">1</span>,dim=-<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b.squeeze())</span><br><span class="line"><span class="built_in">print</span>(c.squeeze())</span><br></pre></td></tr></table></figure>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>])</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>
</blockquote>
<h2><span id="8-tensorexpand-方法">8. tensor.expand() 方法</span></h2>
<p><code>tensor.expand()</code>函数可以将维度值包含 1
的<code>Tensor</code>（如：<code>torch.Size([1, n])</code>或者<code>torch.Size([n, 1])</code>）的维度进行扩展。其具体的扩展规则如下：</p>
<blockquote>
<ol type="1">
<li>只能对<strong>维度值包含 1
的张量Tensor进行扩展</strong>，即：Tensor的size必须满足：<code>torch.Size([1, n])</code>或者
<code>torch.Size([n, 1])</code>。</li>
<li><strong>只能对维度值等于 1
的那个维度进行扩展</strong>，无需扩展的维度务必保持维度值不变，或者置为-1，否则，报错。（简言之，只要是单维度均可进行扩展，但是若非单维度会报错。）</li>
<li>扩展的Tensor<strong>不会分配新的内存</strong>，只是原来的基础上创建新的视图并返回；</li>
<li>新扩展维度的取值范围为：− 1 以 及 [ 1 , + ∞ ] 区 间 内 的 任 意 整
数 -1以及[1, +∞]区间内的任意整数−1以及[1,+∞]区间内的任意整数，例如：将
torch.Size([1, n]) 扩展为torch.Size([m, n])时，新扩展维度 m
的可能取值为-1，或者 m ≥ 1的任意整数；</li>
<li>只能对张量Tensor进行维度扩展，而<strong>不能降维</strong>；否则，报错。</li>
<li>tensor通过.expand()函数扩展某一维度后，tensor自身不会发生变化。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">2</span>], [<span class="number">3</span>], [<span class="number">4</span>]])   <span class="comment"># 创建size为3行1列的张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;a:\n&quot;</span>, a)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">a:</span><br><span class="line"> tensor([[<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>]])</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># （1）将torch.Size([3, 1])扩展为torch.Size([3, 2])</span></span><br><span class="line">a.expand(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>

    </article>
    <!-- license -->
    
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href="/2022/11/04/Pytorch-nn-Embedding/" title="Pytorch-nn-Embedding">
                    <div class="nextTitle">Pytorch-nn-Embedding</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href="/2022/10/27/Pytorch-nn-Dropout/" title="(Pytorch) nn.Dropout">
                    <div class="prevTitle">(Pytorch) nn.Dropout</div>
                </a>
            
        </li>
    </ul>
    <!-- comment -->
    
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->


            

            

            

            <!-- utteranc评论 -->


            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->


            
            

            

        </div>
    
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    
    <!-- Mathjax -->
    
</main>

                <!-- profile -->
                
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
    
        <div class="social">
            
    
        
            
                <a href="mailto:1123836520@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/vacuum-cup" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                    
                    <img class="profile-qr" src="/assets/wechat.png" />
                </span>
            
        
    
        
            
                <span class="iconfont-archer qq" title=qq>
                    
                    <img class="profile-qr" src="/assets/qq.jpg" />
                </span>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    


        </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- website approve for Chinese user -->
    
    <!-- 不蒜子  -->
    
        <div class="busuanzi-container">
            
             
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
            
        </div>
    	
</footer>

        </div>
        <!-- toc -->
        
            <div class="toc-wrapper toc-wrapper-loding" style=







    top:50vh;

>
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">1. torch.chunk方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">2. torch.cumsum方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">3. torch.cat方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">4. torch.stack方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">5. torch.max方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">6.
squeeze与unsqueezef方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">7. torch.split方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">8. tensor.expand() 方法</span></a></li></ol>
            </div>
        
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 29
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
        
            
            
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/03</span>
            <a class="archive-post-title" href="/2023/03/03/numpy%E9%83%A8%E5%88%86%E6%96%B9%E6%B3%95/">numpy部分方法</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">02/23</span>
            <a class="archive-post-title" href="/2023/02/23/Python-%E5%8F%AF%E5%8F%98%E5%8F%82%E6%95%B0-%E4%BC%A0%E5%85%A5%E5%8F%82%E6%95%B0/">Python-可变参数-传入参数</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/02</span>
            <a class="archive-post-title" href="/2023/01/02/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0-%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/">小样本学习-关系抽取</a>
        </li>
    
        
            
            
                
                </ul>
            
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/19</span>
            <a class="archive-post-title" href="/2022/12/19/2022-12-19%E7%BB%84%E4%BC%9A%E8%AE%A8%E8%AE%BA/">2022-12-19组会讨论</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/07</span>
            <a class="archive-post-title" href="/2022/12/07/Tensorflow-SimpleRNN/">Tensorflow-SimpleRNN</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/05</span>
            <a class="archive-post-title" href="/2022/12/05/Tensorflow-Tensorboard/">Tensorflow-Tensorboard</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/05</span>
            <a class="archive-post-title" href="/2022/12/05/Tensorflow-%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">Tensorflow-建造第一个神经网络</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/04</span>
            <a class="archive-post-title" href="/2022/12/04/Tensorflow%E5%9F%BA%E7%A1%80/">Tensorflow基础</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/01</span>
            <a class="archive-post-title" href="/2022/12/01/Pytorch-data-TensorDataset/">Pytorch-data-TensorDataset</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/28</span>
            <a class="archive-post-title" href="/2022/11/28/2022-11-28%E7%BB%84%E4%BC%9A%E8%AE%B0%E5%BD%95/">2022-11-28组会记录</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/28</span>
            <a class="archive-post-title" href="/2022/11/28/Diffusion-Model/">Diffusion Model</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/26</span>
            <a class="archive-post-title" href="/2022/11/26/%E6%8C%87%E6%95%B0%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%EF%BC%88EMA%EF%BC%89/">指数移动平均（EMA）</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/26</span>
            <a class="archive-post-title" href="/2022/11/26/Python-call-%E6%96%B9%E6%B3%95/">Python 中的特殊成员(属性和方法)</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/26</span>
            <a class="archive-post-title" href="/2022/11/26/Pytorch-nn-BCEWithLogitsLoss/">(Pytorch)nn.BCEWithLogitsLoss</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/2022%E5%B9%B411%E6%9C%8821%E6%97%A5%E7%BB%84%E4%BC%9A/">2022年11月21日组会</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/%E7%A0%94%E7%A9%B6%E6%80%9D%E8%B7%AF%E9%9A%8F%E7%AC%94/">研究思路随笔</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">多通道卷积网络</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/">迁移学习</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/TCN-%E6%97%B6%E9%97%B4%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/">TCN-时间卷积网络</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/21</span>
            <a class="archive-post-title" href="/2022/11/21/Transformer%E4%B8%8ELSTM%E4%BC%98%E5%8A%A3%E5%88%86%E6%9E%90/">Transformer与LSTM优劣分析</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/16</span>
            <a class="archive-post-title" href="/2022/11/16/Bert%E4%B8%AD%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E5%90%84%E9%A1%B9%E5%BC%82%E6%80%A7/">Bert中的词向量各项异性</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/15</span>
            <a class="archive-post-title" href="/2022/11/15/11%E6%9C%8814%E7%BB%84%E4%BC%9A/">11月14日组会</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/13</span>
            <a class="archive-post-title" href="/2022/11/13/Pandas-excel-%E6%95%B0%E6%8D%AE%E8%BF%BD%E5%8A%A0/">Pandas-excel-数据追加</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/13</span>
            <a class="archive-post-title" href="/2022/11/13/Pytorch-data-DataLoader%E4%BD%BF%E7%94%A8/">Pytorch-data-DataLoader使用</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/09</span>
            <a class="archive-post-title" href="/2022/11/09/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">对比学习</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/04</span>
            <a class="archive-post-title" href="/2022/11/04/Pytorch-nn-Embedding/">Pytorch-nn-Embedding</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/28</span>
            <a class="archive-post-title" href="/2022/10/28/Pytorch-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/">(Pytorch)中的一些基本方法</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/27</span>
            <a class="archive-post-title" href="/2022/10/27/Pytorch-nn-Dropout/">(Pytorch) nn.Dropout</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/24</span>
            <a class="archive-post-title" href="/2022/10/24/my-first-blog/">my first blog</a>
        </li>
    
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
        
            <span class="sidebar-tag-name" data-tags="private">
                <span class="iconfont-archer">&#xe606;</span>
                private
            </span>
        
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="notebook">
            <span class="iconfont-archer">&#xe60a;</span>
            notebook
        </span>
    
        <span class="sidebar-category-name" data-categories="nlp">
            <span class="iconfont-archer">&#xe60a;</span>
            nlp
        </span>
    
        <span class="sidebar-category-name" data-categories="Pandas">
            <span class="iconfont-archer">&#xe60a;</span>
            Pandas
        </span>
    
        <span class="sidebar-category-name" data-categories="深度学习基础">
            <span class="iconfont-archer">&#xe60a;</span>
            深度学习基础
        </span>
    
        <span class="sidebar-category-name" data-categories="Python">
            <span class="iconfont-archer">&#xe60a;</span>
            Python
        </span>
    
        <span class="sidebar-category-name" data-categories="Pytorch">
            <span class="iconfont-archer">&#xe60a;</span>
            Pytorch
        </span>
    
        <span class="sidebar-category-name" data-categories="bert">
            <span class="iconfont-archer">&#xe60a;</span>
            bert
        </span>
    
        <span class="sidebar-category-name" data-categories="Tensorflow">
            <span class="iconfont-archer">&#xe60a;</span>
            Tensorflow
        </span>
    
        <span class="sidebar-category-name" data-categories="contrastive">
            <span class="iconfont-archer">&#xe60a;</span>
            contrastive
        </span>
    
        <span class="sidebar-category-name" data-categories="小样本学习">
            <span class="iconfont-archer">&#xe60a;</span>
            小样本学习
        </span>
    
        <span class="sidebar-category-name" data-categories="transformer">
            <span class="iconfont-archer">&#xe60a;</span>
            transformer
        </span>
    
        <span class="sidebar-category-name" data-categories="numpy">
            <span class="iconfont-archer">&#xe60a;</span>
            numpy
        </span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://github.com/vacuum-cup/vacuum-cup.github.io",
        root: siteMetaRoot,
        author: "xnf"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->


        <!-- main func -->
        <script src="/scripts/main.js?v=20211217"></script>
        <!-- dark mode -->
        <script src="/scripts/dark.js?v=20211217"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" defer></script>
        <!-- algolia -->
        
        <!-- busuanzi -->
        
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        
        <!-- CNZZ -->
        
        <!-- async load share.js -->
        
            <script src="/scripts/share.js?v=20211217" async></script>
        
        <!-- mermaid -->
        
            <script src='https://cdn.jsdelivr.net/npm/mermaid@8.x/dist/mermaid.min.js'></script>
            <script>
                if (window.mermaid) {
                    mermaid.initialize({theme: 'dark'});
                }
            </script>
        
    </body>
</html>
