{"meta":{"title":"Hexo","subtitle":"","description":"","author":"xnf","url":"https://github.com/vacuum-cup/vacuum-cup.github.io","root":"/"},"pages":[{"title":"[404]","date":"2022-11-15T13:26:07.499Z","updated":"2022-11-15T13:26:07.499Z","comments":true,"path":"404.html","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/404.html","excerpt":"","text":"页面走丢了~"},{"title":"About","date":"2022-11-15T07:16:27.000Z","updated":"2022-11-15T13:24:00.378Z","comments":true,"path":"about/index.html","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/about/index.html","excerpt":"","text":"隐姓埋名的小脑斧"}],"posts":[{"title":"Bert中的词向量各项异性","slug":"Bert中的词向量各项异性","date":"2022-11-16T01:43:20.000Z","updated":"2022-11-16T02:50:38.644Z","comments":true,"path":"2022/11/16/Bert中的词向量各项异性/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/16/Bert%E4%B8%AD%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E5%90%84%E9%A1%B9%E5%BC%82%E6%80%A7/","excerpt":"","text":"概念 各向异性(Anisotropic) 的概念在BERT-flow的文章中有明确的定义： “Anisotropic” means word embeddings occupy a narrow cone in the vector space. 翻译过来就是：“各向异性”表示词嵌入在向量空间中占据了一个狭窄的圆锥形体。 在向量空间中： 各项异性就是分布与分布方向有关系，而各项同性就是各个方向都一样，以二维空间为例： 部分学者发现，Transformer学到的词向量在空间的分布如下： Bert与GPT-2中也存在类似的情况。由上图可以知道模型学到的向量分布是各项异性的。 各项异性的缺点各向异性就有个问题，那就是最后学到的向量都挤在一起，彼此之间计算余弦相似度都很高，并不是一个很好的表示。一个好的向量表示应该同时满足Alignment 和 uniformity，前者表示相似的向量距离应该相近，后者就表示向量在空间上应该尽量均匀，最好是各向同性的。 左图是理想的表示，右图则有各向异性的缺点。 应对方法1. 映射为各向同性BERT-flow的工作就是将原来的分布校准为高斯分布。标准的高斯分布就是各向同性的。 类似的还有whitening操作。大概流程就是根据SVD分解的结果，旋转缩放后得到一个标准正态分布。 2. 消除主成分参见论文： A Simple but Tough-to-Beat Baseline for Sentence Embeddings All-but-the-Top: Simple and Effective Postprocessing for Word Representations 3. 正则化参见论文： Representation Degeneration Problem in Training Natural Language Generation Models 本文选自： 作者：看图学链接：https://www.zhihu.com/question/460991118/answer/2353153090来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"nlp","slug":"nlp","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/"},{"name":"bert","slug":"nlp/bert","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/bert/"}],"tags":[]},{"title":"Pandas-excel-数据追加","slug":"Pandas-excel-数据追加","date":"2022-11-13T09:14:00.000Z","updated":"2022-11-15T13:31:00.021Z","comments":true,"path":"2022/11/13/Pandas-excel-数据追加/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/13/Pandas-excel-%E6%95%B0%E6%8D%AE%E8%BF%BD%E5%8A%A0/","excerpt":"","text":"不同于 csv ，无法直接通过mode=”a”，进行追加。 具体方法: 读取要追加数据的excel文件,然后与要追加的数据组合一起保存到原文件中 12345678#--coding-- utf-8--import pandasdef append_to_excel(filepath,dataframe)-&gt;None: writer=pandas.ExcelWriter(filepath,mode=&#x27;w&#x27;)#这里的mode需要用w模式，a模式会产生新的sheet data=pandas.read_excel(writer,index_col=None,header=None) data.to_excel(writer,startrow=0,index=None,header=None,sheet_name=&#x27;sheet1&#x27;) dataframe.to_excel(writer,startrow=data.shape[0],index=None,header=None,sheet_name=&#x27;sheet1&#x27;) writer.save() 感谢： (43条消息) python pandas excel同一个sheet追加不覆盖追加数据_Demnok Lannik的博客-CSDN博客_python panada 追加sheet","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pandas/"}],"tags":[]},{"title":"Pytorch-data-DataLoader使用","slug":"Pytorch-data-DataLoader使用","date":"2022-11-13T06:44:06.000Z","updated":"2022-11-15T13:31:27.438Z","comments":true,"path":"2022/11/13/Pytorch-data-DataLoader使用/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/13/Pytorch-data-DataLoader%E4%BD%BF%E7%94%A8/","excerpt":"","text":"简介 torch.utils.data.DataLoader主要是对数据进行batch的划分，除此之外，特别要注意的是输入进函数的数据一定得是可迭代的。如果是自定的数据集的话可以在定义类中用def__len__、def__getitem__定义。 使用DataLoader的好处是，可以快速的迭代数据。 示例 123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.utils.data as Datatorch.manual_seed(1) # reproducible BATCH_SIZE = 5 # 批训练的数据个数 x = torch.linspace(1, 10, 10) # x data (torch tensor)y = torch.linspace(10, 1, 10) # y data (torch tensor) # 先转换成 torch 能识别的 Datasettorch_dataset = Data.TensorDataset(x, y) # 把 dataset 放入 DataLoaderloader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # 要不要打乱数据 (打乱比较好) num_workers=2, # 多线程来读数据) for epoch in range(3): # 训练所有!整套!数据 3 次 for step, (batch_x, batch_y) in enumerate(loader): # 每一步 loader 释放一小批数据用来学习 # 假设这里就是你训练的地方... # 打出来一些数据 print(&quot;&#x27;Epoch: &#x27;&quot;, epoch, &quot;&#x27;| Step: &#x27;&quot;, step, &quot;&#x27;| batch x: &#x27;&quot;,batch_x.numpy(), \\ &quot;&#x27;| batch y: &#x27;&quot;, batch_y.numpy()) &quot;&quot;&quot;Epoch: 0 | Step: 0 | batch x: [ 6. 7. 2. 3. 1.] | batch y: [ 5. 4. 9. 8. 10.]Epoch: 0 | Step: 1 | batch x: [ 9. 10. 4. 8. 5.] | batch y: [ 2. 1. 7. 3. 6.]Epoch: 1 | Step: 0 | batch x: [ 3. 4. 2. 9. 10.] | batch y: [ 8. 7. 9. 2. 1.]Epoch: 1 | Step: 1 | batch x: [ 1. 7. 8. 5. 6.] | batch y: [ 10. 4. 3. 6. 5.]Epoch: 2 | Step: 0 | batch x: [ 3. 9. 2. 6. 7.] | batch y: [ 8. 2. 9. 5. 4.]Epoch: 2 | Step: 1 | batch x: [ 10. 4. 8. 1. 5.] | batch y: [ 1. 7. 3. 10. 6.]&quot;&quot;&quot;“”如果改变batch大小每次迭代数据不够batch,则函数就会把剩下的数据输出。“”","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"对比学习","slug":"对比学习","date":"2022-11-09T01:23:17.000Z","updated":"2022-11-15T13:30:10.687Z","comments":true,"path":"2022/11/09/对比学习/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/09/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"对比学习属于无监督或者自监督学习 目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。而如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键。 目前出现的一些方法： 从防止模型坍塌的不同方法角度，我们可大致把现有方法划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。","categories":[{"name":"contrastive","slug":"contrastive","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/contrastive/"}],"tags":[]},{"title":"Pytorch-nn-Embedding","slug":"Pytorch-nn-Embedding","date":"2022-11-04T02:51:22.000Z","updated":"2022-11-15T13:32:52.492Z","comments":true,"path":"2022/11/04/Pytorch-nn-Embedding/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/04/Pytorch-nn-Embedding/","excerpt":"","text":"简述 123torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None) 其为一个简单的存储固定大小的词典的嵌入向量的查找表，意思就是说，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。 输入为一个编号列表，输出为对应的符号嵌入向量列表。 参数说明 num_embeddings (python:int) – 词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999） embedding_dim (python:int)– 嵌入向量的维度，即用多少维来表示一个符号。 padding_idx (python:int, optional) – 填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0） max_norm (python:float, optional) – 最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化 norm_type (python:float, optional) – 指定利用什么范数计算，并用于对比max_norm，默认为2范数。 scale_grad_by_freq (boolean, optional) – 根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False. sparse (bool, optional) – 若为True,则与权重矩阵相关的梯度转变为稀疏张量。 实例 123456789101112131415161718192021222324&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3&gt;&gt;&gt; embedding = nn.Embedding(10, 3)&gt;&gt;&gt; # a batch of 2 samples of 4 indices each&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])&gt;&gt;&gt; embedding(input)tensor([[[-0.0251, -1.6902, 0.7172], [-0.6431, 0.0748, 0.6969], [ 1.4970, 1.3448, -0.9685], [-0.3677, -2.7265, -0.1685]], [[ 1.4970, 1.3448, -0.9685], [ 0.4362, -0.4004, 0.9400], [-0.6431, 0.0748, 0.6969], [ 0.9124, -2.3616, 1.1151]]]) &gt;&gt;&gt; # example with padding_idx&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])&gt;&gt;&gt; embedding(input)tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.1535, -2.0309, 0.9315], [ 0.0000, 0.0000, 0.0000], [-0.1655, 0.9897, 0.0635]]])","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"(Pytorch)中的一些基本方法","slug":"Pytorch-中的一些基本方法","date":"2022-10-28T03:18:51.000Z","updated":"2022-11-20T00:54:55.368Z","comments":true,"path":"2022/10/28/Pytorch-中的一些基本方法/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/28/Pytorch-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/","excerpt":"1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法","text":"1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法 1. torch.chunk方法1Tensor.chunk(chunks,dim=0) # 可以参考torch.chunk() torch.chunk tensor (Tensor) – the tensor to split chunks (int) - number of chunks to return（分割的块数） dim (int) - dimension along which to split the tensor（沿着哪个轴分块） 实例： 2. torch.cumsum方法返回 输入张量指定维度累加和 1torch.cumsum(input, dim, *, dtype=None, out=None) → Tensor 参数： input (Tensor) 输入张量 dim (int) 操作的维度 计算原理: 3. torch.cat方法在给定维数中连接给定序列的 seq 张量。所有张量必须具有相同的形状(连接维数除外)或为空。 1torch.cat(tensors, dim=0, *, out=None) → Tensor 实例： 12345678910111213141516&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) 4. torch.stack方法沿着一个新的维数连接一系列张量。 所有张量的大小必须相同。 1torch.stack(tensors, dim=0, *, out=None) → Tensor # 插入了一个维度 实例： 1234567891011import torchl = 4tag = torch.FloatTensor(l, l, 2).fill_(0)tag1 = torch.FloatTensor(l, l, 2).fill_(5)tag2 = torch.FloatTensor(l, l, 2).fill_(2)tags = torch.stack([tag,tag1,tag2],dim=0)display(tags.shape)tags = torch.stack([tag,tag1,tag2],dim=1)tags.shape output: 12torch.Size([3, 4, 4, 2])torch.Size([4, 3, 4, 2]) 5. torch.max方法方式一： 1torch.max(input) → Tensor 返回所有元素的最大值 实例： 12345&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; atensor([[ 0.6763, 0.7445, -2.2369]])&gt;&gt;&gt; torch.max(a)tensor(0.7445) 方式二： 1torch.max(input, dim, keepdim=False, *, out=None) 返回一个命名元组(values, indexes)，其中values是给定维度dim中输入张量的每一行的最大值。而indexes是找到的每个最大值的索引位置(argmax)。 实例： 12345678&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]])&gt;&gt;&gt; torch.max(a, 1)torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 方式二，个人理解： ​ 给定一个张量 t 和维度d，设 t 的维度为 x*y*z 最后的返回值values（仅表示思路 ）: 12345d = 0 # 举例output = torch.zeros(y,z)for i in range(y): for j in range(z): output[i][j] = max(t[:][i][j]) 6. squeeze与unsqueezef方法给tensor删除或者添加维度为1的维度 squeeze() 方法 在pytorch中，用torch.squeeze()函数或者tensor的自身成员函数squeeze()去除维度为1的维度。 使用示例: 12345678x = torch.randn(3,1,2,4,1)print(x.shape)x_ = x.squeeze() # 默认删除所大小为1 的维度print(x_.shape)y = torch.squeeze(x,dim=1) # 指定维度print(y.shape)z = torch.squeeze(x,dim=2) # dim=2 维度不是1 无法去除 但不会报错print(z.shape) 1234torch.Size([3, 1, 2, 4, 1])torch.Size([3, 2, 4])torch.Size([3, 2, 4, 1])torch.Size([3, 1, 2, 4, 1]) unsqueeze() 方法 在pytorch中，用自带的torch.unsqueeze()和tensor的成员函数unsqueeze()可以为tensor添加维度为1的维度 1234e = torch.unsqueeze(x, dim=0) # 在第一维度添加维度print(e.shape)f = x.unsqueeze(dim=0)print(f.shape) 12torch.Size([1, 3, 1, 4, 1, 2])torch.Size([1, 3, 1, 4, 1, 2])","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"(Pytorch) nn.Dropout","slug":"Pytorch-nn-Dropout","date":"2022-10-27T09:04:16.000Z","updated":"2022-11-15T13:32:14.476Z","comments":true,"path":"2022/10/27/Pytorch-nn-Dropout/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/27/Pytorch-nn-Dropout/","excerpt":"","text":"目的： 为了防止过拟合 具体用法 1nn.Dropout(0.2) #表示每个输入的神经元有 0.2的概率被设为0 从下图中也可以看到，其他的数 除了（1-0.3） 补充说明: Dropout 只能用于训练部分，不可用于测试 一般用在全连接神经网络映射层之后 还可用于将Tensor中的部分值设为0 如上图","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"my first blog","slug":"my-first-blog","date":"2022-10-24T01:14:45.000Z","updated":"2022-11-20T01:28:36.846Z","comments":true,"path":"2022/10/24/my-first-blog/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/24/my-first-blog/","excerpt":"","text":"我的第一个博客 你好世界！ 博客模板： fi3ework/hexo-theme-archer: 🎯 A smart and modern theme for Hexo. (github.com) memaid测试 graph LR 1(CIR-Lab) --> 1.1(实验室共享文件) -->小组A学习资料 1.1--> 小组B学习资料 1.1--> 研究生课程学习与考试资料等 1.1--> 文件互传 style 1.1 fill:#0f0,stroke:#333,stroke-width:4px 1 --> 1.2(实验室项目资料) -->项目A资料 style 1.2 fill:#f91,stroke:#333,stroke-width:4px 1.2--> 软著资料 1.2--> 专利资料 1.2--> 论文资料 latex公式测试 \\[ \\begin{equation} \\left\\{ \\begin{array}{lr} x=\\dfrac{3\\pi}{2}(1+2t)\\cos(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\\\ y=s, &amp; 0 \\leq s \\leq L,|t| \\leq1. \\\\ z=\\dfrac{3\\pi}{2}(1+2t)\\sin(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\end{array} \\right. \\end{equation} \\]","categories":[],"tags":[]}],"categories":[{"name":"nlp","slug":"nlp","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/"},{"name":"bert","slug":"nlp/bert","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/bert/"},{"name":"Pandas","slug":"Pandas","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pandas/"},{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"},{"name":"contrastive","slug":"contrastive","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/contrastive/"}],"tags":[]}