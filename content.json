{"meta":{"title":"Hexo","subtitle":"","description":"","author":"xnf","url":"https://github.com/vacuum-cup/vacuum-cup.github.io","root":"/"},"pages":[{"title":"About","date":"2022-11-15T07:16:27.000Z","updated":"2022-11-15T13:24:00.378Z","comments":true,"path":"about/index.html","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/about/index.html","excerpt":"","text":"隐姓埋名的小脑斧"},{"title":"[404]","date":"2022-11-15T13:26:07.499Z","updated":"2022-11-15T13:26:07.499Z","comments":true,"path":"404.html","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/404.html","excerpt":"","text":"页面走丢了~"}],"posts":[{"title":"Tensorflow-SimpleRNN","slug":"Tensorflow-SimpleRNN","date":"2022-12-07T00:17:00.000Z","updated":"2022-12-07T11:27:04.712Z","comments":true,"path":"2022/12/07/Tensorflow-SimpleRNN/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/12/07/Tensorflow-SimpleRNN/","excerpt":"","text":"","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Tensorflow/"}],"tags":[]},{"title":"Tensorflow-Tensorboard","slug":"Tensorflow-Tensorboard","date":"2022-12-05T09:41:22.000Z","updated":"2022-12-06T03:30:15.854Z","comments":true,"path":"2022/12/05/Tensorflow-Tensorboard/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/12/05/Tensorflow-Tensorboard/","excerpt":"","text":"简介 可视化神经网络结构实例 搭建图纸 可视化训练过程 简介 tensorboard可以可视化tensorflow构造出的神经网络，需要结合浏览器使用，主要是兼容\"Google Chrome\" 除此之外，Tensorboard还可以可视化训练过程（biases变化过程）。 可视化神经网络结构实例 搭建图纸 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import tensorflow as tfimport numpy as npx_data = np.linspace(-1,1,300,dtype=np.float32)[:,np.newaxis]noise = np.random.normal(0,0.05,x_data.shape).astype(np.float32)y_data = np.square(x_data)+noise# 从输入开始## 通过 下述方式可以将xs ys包裹起来形成一个大图层with tf.name_scope(&quot;inputs&quot;) :#给输入占位命名，之后会在可视化图中显示 xs = tf.placeholder(tf.float32,[None,1],name=&quot;x_in&quot;) # ys = tf.placeholder(tf.float32,[None,1],name=&quot;y_in&quot;) ## 编辑layerdef add_layer(inputs,in_size,out_size,activation_function=None): with tf.name_scope(&quot;layer&quot;): with tf.name_scope(&quot;Weights&quot;): Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=&quot;W&quot;) with tf.name_scope(&quot;Biases&quot;): Biases = tf.Variable(tf.zeros([1,out_size])+0.1,name=&quot;b&quot;) with tf.name_scope(&quot;Wx_add_b&quot;): Wx_add_b =tf.add(tf.matmul(inputs,Weights),Biases) if activation_function==None: # 使用tensor中的提供的激活函数会默认添加名称 output=Wx_add_b else: output = activation_function(Wx_add_b) return output# 实例化层l1 = add_layer(xs,1,10,tf.nn.relu)prediction = add_layer(l1,10,1,None)# loss 部分with tf.name_scope(&quot;loss&quot;): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=1))# train 部分with tf.name_scope(&quot;train&quot;): train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)# 需要使用一个tf.summary.FileWriter() 将上述绘图，保存到一个目录中，以便后续在浏览器中浏览sesssess = tf.Session()wariter=tf.summary.FileWriter(&quot;log/&quot;,sess.graph)# 最后在终端中输入如下命令# tensorboard --logdir log 通过localhost:6066 可以看到绘图： 可视化训练过程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import tensorflow as tfimport numpy as np ## make up some datax_data= np.linspace(-1, 1, 300, dtype=np.float32)[:,np.newaxis]noise= np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data= np.square(x_data) -0.5+ noisewith tf.name_scope(&quot;inputs&quot;) :#给输入占位命名，之后会在可视化图中显示 xs = tf.placeholder(tf.float32,[None,1],name=&quot;x_in&quot;) # ys = tf.placeholder(tf.float32,[None,1],name=&quot;y_in&quot;) ## 编辑layerdef add_layer(inputs,in_size,out_size,n_layer,activation_function=None): layer_name = &quot;layer%s&quot;%n_layer with tf.name_scope(layer_name): with tf.name_scope(&quot;Weights&quot;): Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=&quot;W&quot;) tf.summary.histogram(layer_name+&quot;/weights&quot;,Weights) #第一个参数是图表的名称, 第二个参数是图表要记录的变量 with tf.name_scope(&quot;Biases&quot;): Biases = tf.Variable(tf.zeros([1,out_size])+0.1,name=&quot;b&quot;) tf.summary.histogram(layer_name+&quot;/biases&quot;,Biases) with tf.name_scope(&quot;Wx_add_b&quot;): Wx_add_b =tf.add(tf.matmul(inputs,Weights),Biases) if activation_function==None: # 使用tensor中的提供的激活函数会默认添加名称 output=Wx_add_b else: output = activation_function(Wx_add_b) tf.summary.histogram(layer_name+&quot;/output&quot;,output) return output# add hidden layerl1= add_layer(xs, 1, 10 ,1, activation_function=tf.nn.relu)# add output layerprediction= add_layer(l1, 10, 1,2, activation_function=None)#设置loss变化图with tf.name_scope(&quot;loss&quot;): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=1)) tf.summary.histogram(&quot;loss&quot;,loss)# train 部分with tf.name_scope(&quot;train&quot;): train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)#合并所有的训练图sess= tf.Session()merged = tf.summary.merge_all() # tensorflow &gt;= 0.12writer = tf.summary.FileWriter(&quot;log/&quot;, sess.graph) # tensorflow &gt;=0.12sess.run(tf.global_variables_initializer()) # 替换成这样就好# 训练数据for i in range(1000): sess.run(train_step, feed_dict=&#123;xs:x_data, ys:y_data&#125;) if i%50 == 0: rs = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;) writer.add_summary(rs, i) 最后在终端中输入如下命令: tensorboard --logdir log 然后在浏览器中localhost:6006就可以看图了 参考链接： Tensorboard 可视化好帮手 1 - Tensorflow | 莫烦Python (yulizi123.github.io)","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Tensorflow/"}],"tags":[]},{"title":"Tensorflow-建造第一个神经网络","slug":"Tensorflow-建造第一个神经网络","date":"2022-12-05T06:40:11.000Z","updated":"2022-12-05T09:31:16.267Z","comments":true,"path":"2022/12/05/Tensorflow-建造第一个神经网络/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/12/05/Tensorflow-%E5%BB%BA%E9%80%A0%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"定义一个添加层 构造神经网络 结果可视化 关于优化器 定义一个添加层 在 Tensorflow 里定义一个添加层的函数可以很容易的添加神经层,为之后的添加省下不少时间. 1234567891011121314151617import tensorflow as tfdef add_layer(inputs,input_size,output_size,activation_func =None): # 一个简单的全连接层 需要定义Weights 和Biases Weights = tf.Variable(tf.random_normal([input_size,output_size])) #随机变量要比全为0好的多 Biases = tf.Variable(tf.zeros([1,output_size])+0.1) Wx_add_b = tf.matmul(inputs,Weights)+Biases # 是否需要激活函数 if activation_func==None: output = Wx_add_b else: output = activation_func(Wx_add_b) return output 构造神经网络 个人理解 就是构建好了一个计算图，然后开始训练 1234567891011121314151617181920212223242526272829303132333435363738# 导入数据import numpy as npx_data = np.linspace(-1,1,300,dtype=np.float32)[:,np.newaxis]noise = np.random.normal(0,0.05,x_data.shape).astype(np.float32)y_data = np.square(x_data)+noise# 利用站位符定义神经网络的输入xs = tf.placeholder(tf.float32,[None,1]) # None表示大小是多少都可以ys = tf.placeholder(tf.float32,[None,1])# 搭建网络## 隐藏层l1 = add_layer(xs,1,100,tf.nn.relu)## 输入层prediction = add_layer(l1,100,1,activation_func=None)#计算预测值和真实值之间的误差,对差的平方和再求平均loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=1)) # 关键的一步，如何提升模型准确性train_step = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)#使用变量时需要对其初始化init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)#训练for i in range(1000): # training for j in range(3): # 简单模拟一下batch_size x = x_data[100*j:100*(j+1)] y = y_data[100*j:100*(j+1)] sess.run(train_step,feed_dict=&#123;xs:x,ys:y&#125;) if (i+1) %100 ==0: print(sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;)) 123456789100.0145048910.00593812620.00424028930.00355636680.00312227830.00284836420.0027052360.00258968420.00250186630.002452812 结果可视化 12345678910import matplotlib.pyplot as pltfig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data,y_data)predictionvalues = sess.run(prediction,feed_dict=&#123;xs:x_data&#125;)plt.plot(x_data,predictionvalues,&quot;r-&quot;,lw=5)plt.show() 关于优化器 用的时候查吧 优化器 参考文章： Tensorflow 教程系列 | 莫烦Python (yulizi123.github.io)","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Tensorflow/"}],"tags":[]},{"title":"Tensorflow基础","slug":"Tensorflow基础","date":"2022-12-04T03:11:42.000Z","updated":"2022-12-05T02:48:26.299Z","comments":true,"path":"2022/12/04/Tensorflow基础/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/12/04/Tensorflow%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Tensorflow 计算图 简单示例： Session 会话控制 Variable 变量 Placeholder 传入值 Tensorflow 计算图 Tensorflow 首先要定义神经网络的结构, 然后再把数据放入结构当中去运算和 training. 因为TensorFlow是采用数据流图（data flow graphs）来计算, 所以首先我们得创建一个数据流流图, 然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算。节点（Nodes）在图中表示数学操作,图中的线（edges）则表示在节点间相互联系的多维数据数组, 即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 简单示例： 1234567891011121314151617181920212223242526272829import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3# model builtWeights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases# calculate lossloss = tf.reduce_mean(tf.square(y-y_data))# optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# 上面只是init = tf.global_variables_initializer() # 初始化之前定义的所有的 Variable ！sess = tf.Session() # 创建一个会话 后面会介绍啥是会话sess.run(init) # Very importantfor step in range(200): sess.run(train) if (step+1) % 20 == 0: print(step+1, sess.run(Weights), sess.run(biases)) 12345678910110 [0.02634814] [0.48489285]20 [0.07239345] [0.31540158]40 [0.09434778] [0.30315337]60 [0.09884276] [0.30064565]80 [0.09976308] [0.3001322]100 [0.09995149] [0.30002707]120 [0.09999008] [0.30000556]140 [0.09999797] [0.30000114]160 [0.09999958] [0.30000025]180 [0.09999991] [0.30000007]200 [0.09999991] [0.30000007] Session 会话控制 Session 是Tensorflow 为了控制,和输出文件的执行的语句。 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分。 12345678910111213141516171819202122import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2)#因为 product 不是直接计算的步骤, 所以我们会要使用 Session 来激活 product 并得到计算结果. #有两种形式使用会话控制 Session# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2) 12[[12]][[12]] Variable 变量 在tensorflow中定义了某字符串是变量，它才是变量，区别于python。 定义语法： tf.Variable() 123456789101112131415161718192021import tensorflow as tfstate = tf.Variable(0,name=&quot;counter&quot;)one = tf.constant(1)#定义加法步骤(注: 没有直接计算)new_value = tf.add(state,one) # 将State 更新成 new_value update = tf.assign(state,new_value) # 需要执行一下#如果定义了 Variable 就一定要initializerinit = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for _ in range(3): sess.run(update) # 数据更新也是需要run一下的 print(sess.run(new_value)) 123234 Placeholder 传入值 placeholder是Tensorflow中的占位符，暂时储存变量。 Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(***, feed_dict=&#123;input: **&#125;). 123456789101112import tensorflow as tf #在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# 定义乘法运算output = tf.multiply(input1,input2)with tf.Session() as sess: print(sess.run(output,feed_dict=&#123;input1:6,input2:8&#125;)) # print(sess.run(output,feed_dict=&#123;input1:[6],input2:[8]&#125;)) # 1248.0[48.] 参考资料: Tensorflow 教程系列 | 莫烦Python (yulizi123.github.io)","categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Tensorflow/"}],"tags":[]},{"title":"Pytorch-data-TensorDataset","slug":"Pytorch-data-TensorDataset","date":"2022-12-01T13:00:49.000Z","updated":"2022-12-01T13:22:49.430Z","comments":true,"path":"2022/12/01/Pytorch-data-TensorDataset/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/12/01/Pytorch-data-TensorDataset/","excerpt":"","text":"关于 TensorDataset方法的说明 TensorDataset 可以用来对 tensor 进行打包，就好像 python 中的 zip 功能。该类通过每一个 tensor 的第一个维度进行索引。因此，该类中的 tensor 第一维度必须相等。 实例： 12345678from torch.utils.data import TensorDatasetimport torcha = torch.tensor([[11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99], [11, 22, 33], [44, 55, 66], [77, 88, 99]])b = torch.tensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2])train_ids = TensorDataset(a, b) print(train_ids[0:2]) 12(tensor([[11, 22, 33], [44, 55, 66]]), tensor([0, 1])) 参考文章： TensorDataset_anshiquanshu的博客-CSDN博客_tensordataset","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"Diffusion Model","slug":"Diffusion-Model","date":"2022-11-28T02:32:47.000Z","updated":"2022-11-28T12:03:15.805Z","comments":true,"path":"2022/11/28/Diffusion-Model/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/28/Diffusion-Model/","excerpt":"","text":"简介 扩散过程 定义 重参数技巧得到迭代公式 逆扩散过程 基于扩散模型的目标检测框架 类别引导问题？ 简介 扩散模型是Encoder-Decoder架构的生成模型，分为扩散阶段和逆扩散阶段。 在扩散阶段，通过不断对原始数据添加噪声，使数据从原始分布变为我们期望的分布，例如通过不断添加高斯噪声将原始数据分布变为正态分布。 在逆扩散阶段，使用神经网络将数据从正态分布恢复到原始数据分布 Diffusion model模型分为扩散过程和逆扩散过程，扩散过程通过对原始数据不断加入高斯噪音，使原始数据变为高斯分布的数据，即从\\(X_0-&gt;X_T\\)；逆扩散过程通过高斯噪声还原图片，即\\(X_T-&gt;X_0\\) 扩散过程 定义 在设定扩散过程是一个马尔可夫链的条件下，向原始信息中不断添加高斯噪声，每一步添加高斯噪声的过程是从\\(X_{t-1}-&gt;X_T\\) 于是定义公式： 表示，\\(X_{t-1}-&gt;X_T\\) 是一个以\\(\\sqrt{1-\\beta_t}x_{t-1}为均值\\)，\\(\\beta_t\\)为方差的高斯分布变换 重参数技巧得到迭代公式 重参数 \\(X_t\\)表示t时刻的数据分布 \\(Z_t\\)表示t时刻添加的高斯噪音，一般固定是均值为0方差为1的高斯分布 \\(\\sqrt{1-\\beta_t}X_{t-1}\\)表示前一时刻分布的均值 \\(\\sqrt{\\beta_t}\\)表示当前时刻分布的标准差。 \\(\\beta_t\\)是预先设定的\\(0-1\\)之间的常量，所以扩散过程是不含参的。 逆扩散过程 扩散过程是将数据噪音化，反向过程就是一个去噪的过程。逆向扩散过程中，我们将以高斯噪声 \\(T∼N(0,I)\\)作为输入，从 \\((x_{t−1}|x_t)\\) 中采样，推断并重构出真实样本。这里需注意，如果 \\(β_t\\) 足够小，\\(q(x_{t−1}|x_t)\\)的采样结果也仍为高斯分布，该值也很难评估，因此难以通过公式求解的方式，来一步步推断出原始的真实分布。这里，大佬们的想法也很直接：整个数据集已经有了，既然无法直接求解，何不尝试训练出一个模型 $p_θ $来对这些噪声的条件概率进行预测呢？既然要做预测，那标签何来？这里便是将前向传播每部生成的真实噪声记录下来作为标签，前向扩散的过程除了推断外，还包含类似该数学模型所用“数据集的构建过程”。在模型做逆向扩散时，即可对前向扩散中所产生的高斯噪声进行预测，并一步一步推断，以还原最初始的样本数据。 基于扩散模型的目标检测框架 原文链接 DiffusionDet，该框架可以直接从一组随机框中检测目标，它将目标检测制定为从噪声框到目标框的去噪扩散过程。这种从 noise-to-box 的方法不需要启发式的目标先验，也不需要可学习查询，这进一步简化了目标候选，并推动了检测 pipeline 的发展。 如下图 1 所示，该研究认为 noise-to-box 范式类似于去噪扩散模型中的 noise-to-image 过程，后者是一类基于似然的模型，通过学习到的去噪模型逐步去除图像中的噪声来生成图像。 类别引导问题？ 参考文献： Diffusion model—扩散模型_原来如此-的博客-CSDN博客_扩散模型 Diffusion Model 扩散模型-通俗易懂+代码讲解]（一） - 知乎 (zhihu.com) 港大&amp;腾讯提出DiffusionDet：第一个用于目标检测的扩散模型 (qq.com) DiffusionDet：基于扩散模型的目标检测框架 - 知乎 (zhihu.com)","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"指数移动平均（EMA）","slug":"指数移动平均（EMA）","date":"2022-11-26T08:24:52.000Z","updated":"2022-11-28T12:15:43.105Z","comments":true,"path":"2022/11/26/指数移动平均（EMA）/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/26/%E6%8C%87%E6%95%B0%E7%A7%BB%E5%8A%A8%E5%B9%B3%E5%9D%87%EF%BC%88EMA%EF%BC%89/","excerpt":"","text":"EMA定义 进一步理解\\(v_t\\) 在深度模型中的应用 关于EMA的作用 EMA定义 指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。 假设我们有n个数据：\\([\\theta_1 ,\\theta_2,...,\\theta_n]\\) 普通的平均数： \\(\\overline{v}=\\frac1n \\sum _{i=1} ^n \\theta_i\\) EMA： \\(v_t = \\beta ·v_{t-1} + (1-\\beta)·\\theta_t\\) ，其中， \\(v_t\\)表示前\\(t\\)条的平均值 ( \\(v_0 =0\\) )， \\(\\beta\\)是加权权重值 (一般设为0.9-0.999)。 EMA 可以看成是过去 \\(1/(1-\\beta)\\)天的 \\(\\theta 值的平均\\) 如： \\(\\beta = 0.9\\) \\(1/(1-\\beta)=10\\) \\(v_t\\)大概表示前10天的平均数据 红线 \\(\\beta = 0.98\\) \\(1/(1-\\beta)=50\\) \\(v_t\\)大概表示前50天的平均数据 绿线 \\(\\beta = 0.5\\) \\(1/(1-\\beta)=2\\) 大\\(v_t\\)概表示前2天的平均数据 黄线 那么\\(\\beta\\)越大，表示考虑的时间长度越长 进一步理解\\(v_t\\) \\(v_t = \\beta ·v_{t-1} + (1-\\beta)·\\theta_t\\) 当 \\(\\beta=0.9\\)，从\\(v_{100}\\)往回写: \\[ v_{100}=0.1\\theta_{100}+0.9v_{99}\\\\ v_{99}=0.1\\theta_{98}+0.9v_{98}\\\\ ... \\] 可得: \\[ v_{100}=0.1\\theta_{100}+0.1*0.9*\\theta_{99}+0.1*(0.9)^2\\theta_{98}+.. \\] 由此可知： \\(v_(100)是\\theta_i的加权求和\\) \\(\\theta\\)前的系数相加逼近1 在深度模型中的应用 \\(v_t = \\beta ·v_{t-1} + (1-\\beta)·\\theta_t\\) \\(\\theta_t\\)：在第t次更新得到的所有参数权重。 \\(v_t\\)：第t次更新的所有参数移动平均数。 \\(\\beta\\)：权重参数。 关于EMA的作用 对于更新n次时普通的参数权重\\(\\theta_n\\): \\[ \\theta_n = \\theta_i - \\sum_{i=1}^{n-1} g_i \\\\ ,g_n 为第n次传播得到的梯度 \\] 对于更新n次时，使用EMA的参数权重\\(v_n\\) \\[ v_n = \\theta_1 -\\sum_{i=1}^{n-1}(1-\\beta^{n-i})g_i \\] 普通的参数权重相当于一直累积更新整个训练过程的梯度，使用EMA的参数权重相当于使用训练过程梯度的加权平均（刚开始的梯度权值很小）。由于刚开始训练不稳定，得到的梯度给更小的权值更为合理，所以EMA会有效。 参考资料： (43条消息) 指数移动平均（EMA）的原理及PyTorch实现_枫林扬的博客-CSDN博客_指数移动平均 (43条消息) EMA(指数移动平均)及其深度学习应用_Kmaeii的博客-CSDN博客_ema指数移动平均值","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Python 中的特殊成员(属性和方法)","slug":"Python-call-方法","date":"2022-11-26T07:56:57.000Z","updated":"2022-11-26T12:47:18.034Z","comments":true,"path":"2022/11/26/Python-call-方法/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/26/Python-call-%E6%96%B9%E6%B3%95/","excerpt":"","text":"__call__()方法 暂无 __call__()方法 Python类中一个非常特殊的实例方法，即 call()。该方法的功能类似于在类中重载 () 运算符，使得类实例对象可以像调用普通函数那样，以“对象名()”的形式使用 实例: 1234567class CLanguage: # 定义__call__方法 def __call__(self,name,add): print(&quot;调用__call__()方法&quot;,name,add)clangs = CLanguage()clangs(&quot;C语言中文网&quot;,&quot;http://c.biancheng.net&quot;) 执行结果 1调用__call__()方法 C语言中文网 http://c.biancheng.net 暂无","categories":[{"name":"Python","slug":"Python","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Python/"}],"tags":[]},{"title":"(Pytorch)nn.BCEWithLogitsLoss","slug":"Pytorch-nn-BCEWithLogitsLoss","date":"2022-11-26T07:08:59.000Z","updated":"2022-11-26T07:54:52.694Z","comments":true,"path":"2022/11/26/Pytorch-nn-BCEWithLogitsLoss/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/26/Pytorch-nn-BCEWithLogitsLoss/","excerpt":"","text":"nn.BCELoss nn.BCEWithLogitsLoss nn.BCELoss 1torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction=&#x27;mean&#x27;) 创建一个衡量目标和输入概率之间的二进制交叉熵的标准 公式 \\[ loss(x,y)= L = \\{l_1,l_2,...,l_N\\} ,\\\\ l_n = -w_n[y_n ·logx_n+(1-y_n)·log(1-x_n)] \\] N 表示batch size ，reduction可以指定模式: \\[ loss(x,y) = \\begin{cases} mean(L), &amp;if\\ reduction\\ =\\ &#39;mean&#39;\\\\ sum(L), &amp;if\\ reduction\\ =\\ &#39;sum&#39; \\end{cases} \\] 实例： 12345678import torchimport torch.nn as nnm = nn.Sigmoid() # 前面需要接 sigmoidloss = nn.BCELoss()input = torch.randn(3, requires_grad=True)target = torch.empty(3).random_(2)output = loss(m(input), target)output.backward() nn.BCEWithLogitsLoss nn.Sigmoid + nn.BCELoss 12345loss = nn.BCEWithLogitsLoss()input = torch.randn(3, requires_grad=True)target = torch.empty(3).random_(2)output = loss(input, target)output.backward()","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"迁移学习","slug":"迁移学习","date":"2022-11-21T10:15:00.000Z","updated":"2022-11-21T12:13:56.608Z","comments":true,"path":"2022/11/21/迁移学习/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/21/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"1. 什么是迁移学习 2. 迁移学习的关键 3. 迁移学习分类 3.1 基于实例的迁移 3.2 基于特征的迁移 3.3 基于共享参数的迁移 4. 深度学习和迁移学习的结合 1. 什么是迁移学习 在某些机器学习场景中，由于直接对目标域从头开始学习成本太高，因此我们期望运用已有的相关知识来辅助尽快地学习新知识。比如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#； 迁移学习（Transfer Learning）通俗来讲就是学会举一反三的能力，通过运用已有的知识来学习新的知识，其核心是找到已有知识和新知识之间的相似性，通过这种相似性的迁移达到迁移学习的目的。 规范化定义： 给定源域 $D_s $和学习任务 $T_s $、目标域 $D_t $和学习任务 \\(T_t\\) ,迁移学习的目的是获取源域 \\(D_s\\)和学习任务 $T_s $中的知识以帮助提升目标域中的预测函数 $f_t(⋅) $的学习，其中 $D_s≠D_t $或者 \\(T_s≠T_t\\) 。 image-20221121192419440 2. 迁移学习的关键 3. 迁移学习分类 基于实例的迁移 基于特征的迁移 基于共享参数的迁移 3.1 基于实例的迁移 基于实例的迁移学习研究的是，如何从源领域中挑选出，对目标领域的训练有用的实例，比如对源领域的有标记数据实例进行有效的权重分配，让源域实例分布接近目标域的实例分布，从而在目标领域中建立一个分类精度较高的、可靠地学习模型。 因为，迁移学习中源领域与目标领域的数据分布是不一致，所以源领域中所有有标记的数据实例不一定都对目标领域有用。戴文渊等人提出的TrAdaBoost算法就是典型的基于实例的迁移。 3.2 基于特征的迁移 基于特征选择的迁移学习算法 如何找出源领域与目标领域之间共同的特征表示，然后利用这些特征进行知识迁移。 基于特征映射的迁移学习算法 如何将源领域和目标领域的数据从原始特征空间映射到新的特征空间中去。 这样，在该空间中，源领域数据与的目标领域的数据分布相同，从而可以在新的空间中，更好地利用源领域已有的有标记数据样本进行分类训练，最终对目标领域的数据进行分类测试。 3.3 基于共享参数的迁移 基于共享参数的迁移研究的是如何找到源数据和目标数据的空间模型之间的共同参数或者先验分布，从而可以通过进一步处理，达到知识迁移的目的，假设前提是，学习任务中的的每个相关模型会共享一些相同的参数或者先验分布。 4. 深度学习和迁移学习的结合 深度学习需要大量的高质量标注数据，Pre-training + fine-tuning 是现在深度学习中一个非常流行的trick，尤其是以图像领域为代表，很多时候会选择预训练的ImageNet对模型进行初始化。 参考资料: (43条消息) 迁移学习概述（Transfer Learning）_zhyuxie的博客-CSDN博客 迁移学习之——什么是迁移学习（Transfer Learning） - 知乎 (zhihu.com)","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"多通道卷积网络","slug":"多通道卷积网络","date":"2022-11-21T10:07:15.000Z","updated":"2022-11-21T10:12:49.005Z","comments":true,"path":"2022/11/21/多通道卷积网络/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/21/%E5%A4%9A%E9%80%9A%E9%81%93%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/","excerpt":"","text":"很容易理解，就放一张图吧 参考资料: (43条消息) 多通道卷积理解_A half moon的博客-CSDN博客_多通道卷积 (43条消息) 多通道图片的卷积_小小川_的博客-CSDN博客_多通道图片","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"TCN-时间卷积网络","slug":"TCN-时间卷积网络","date":"2022-11-21T08:56:49.000Z","updated":"2022-11-21T10:14:02.697Z","comments":true,"path":"2022/11/21/TCN-时间卷积网络/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/21/TCN-%E6%97%B6%E9%97%B4%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/","excerpt":"","text":"1. 引言 2. 时序卷积神经网络（TCN） 2.1 因果卷积 2.2 膨胀卷积 2.3 残差链接 3. TCN优缺点 4. 小结： 参考原文: 1. 引言 传统的卷积神经网络一般认为不太适合时序问题的建模，这主要由于其卷积核大小的限制，不能很好的抓取长时的依赖信息。 特定的卷积神经网络结构也可以达到很好的效果，比如Goolgle提出的用来做语音合成的wavenet，Facebook提出的用来做翻译的卷积神经网络 时序卷积网络（Temporal convolutional network， TCN）的提出是为了是卷积神经网络具备时序特性，与多种RNN结构相对比，发现在多种任务上TCN都能达到甚至超过RNN模型。 2. 时序卷积神经网络（TCN） 分类： 因果卷积(Causal Convolution) 膨胀卷积（Dilated Convolution） 残差链接(Residual Connections) 2.1 因果卷积 即对于上一层t时刻的值，只依赖于下一层t时刻及其之前的值。和传统的卷积神经网络的不同之处在于，因果卷积不能看到未来的数据，它是单向的结构，不是双向的。也就是说只有有了前面的因才有后面的果，是一种严格的时间约束模型，因此被成为因果卷积。 问题: 单纯的因果卷积还是存在传统卷积神经网络的问题，即对时间的建模长度受限于卷积核大小的，如果要想抓去更长的依赖关系，就需要线性的堆叠很多的层。 2.2 膨胀卷积 和传统卷积不同的是，膨胀卷积允许卷积时的输入存在间隔采样，采样率受图中的d控制。 最下面一层的d=1，表示输入时每个点都采样，中间层d=2，表示输入时每2个点采样一个作为输入。一般来讲，越高的层级使用的d的大小越大。所以，膨胀卷积使得有效窗口的大小随着层数呈指数型增长。这样卷积网络用比较少的层，就可以获得很大的感受野。 2.3 残差链接 残差链接被证明是训练深层网络的有效方法，它使得网络可以以跨层的方式传递信息。本文构建了一个残差块来代替一层的卷积。如上图所示，一个残差块包含两层的卷积和非线性映射，在每层中还加入了WeightNorm和Dropout来正则化网络。 3. TCN优缺点 优点 并行性，较之RNN，TCN可以并行处理，不需要顺序处理 感受野灵活 梯度稳定、内存低 缺点： TCN 在迁移学习方面可能没有那么强的适应能力。这是因为在不同的领域，模型预测所需要的历史信息量可能是不同的。因此，在将一个模型从一个对记忆信息需求量少的问题迁移到一个需要更长记忆的问题上时，TCN 可能会表现得很差，因为其感受野不够大。 论文中描述的TCN还是一种单向的结构，在语音识别和语音合成等任务上，纯单向的结构还是相当有用的。但是在文本中大多使用双向的结构，当然将TCN也很容易扩展成双向的结构，不使用因果卷积，使用传统的卷积结构即可。 TCN毕竟是卷积神经网络的变种，虽然使用扩展卷积可以扩大感受野，但是仍然受到限制，相比于Transformer那种可以任意长度的相关信息都可以抓取到的特性还是差了点。TCN在文本中的应用还有待检验。 4. 小结： TCN是一个CNN变体 在语音、文本等方面均有应用 参考原文: TCN-时间卷积网络_满腹的小不甘_静静-DevPress官方社区 (csdn.net)","categories":[{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"Transformer与LSTM优劣分析","slug":"Transformer与LSTM优劣分析","date":"2022-11-21T06:47:48.000Z","updated":"2022-11-21T12:34:09.791Z","comments":true,"path":"2022/11/21/Transformer与LSTM优劣分析/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/21/Transformer%E4%B8%8ELSTM%E4%BC%98%E5%8A%A3%E5%88%86%E6%9E%90/","excerpt":"","text":"1. 关于Transformer劣势？ Transformer缺乏对时间维度的建模，即使有Position Encoding也和LSTM这种天然的时序网络有差距； 因为缺乏时间维度的建模，稍微深层的Transformer编码器的每个位置的输出都会很相似（每层不断的在上一层基础上加权和，感性地理解一下），这一点会导致Transformer在一些要对具体位置分类的任务上表现不好； Transformer的训练trick很多。编码器的层数、attention的head数量、学习率、权重衰减等等都会严重影响模型性能，LSTM这种烦事要少很多； 大家说的Transformer效果好，大多数时候指的使用是预训练的Transformer，也就是BERT、XLNET这些预训练模型。单独用随机参数初始化的Transformer，除了Seq2Seq类模型（生成、翻译），其他领域效果特别好的少； 2. RNN劣势？ RNN结构在NLP中的优势很明显，但是也有一个很明显的缺点，就是RNN本身的序列依赖结构对于大规模并行计算来说相当的不友好，换句话说，就是RNN很难具备高效的并行计算能力。深度学习大火的原因就是因为GPU硬件环境在支持，而RNN因为先天结构的问题（T时刻的隐层状态 \\(S_t\\)还依赖 \\(T-1\\)时刻的隐层状态 \\(S_{t-1}\\)的输出，这是最能体现RNN本质特征的一点），无法充分利用硬件的并行计算能力，这是一个非常非常大的问题！ 3. 如何选？ 该部分的结论来源是：论文“Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures” 语义特征提取能力 Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。 长距离特征捕获能力 CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。 任务综合特征抽取能力 从综合特征抽取能力角度衡量，Transformer显著强于RNN和CNN，而RNN和CNN的表现差不太多，如果一定要在这两者之间比较的话，通常CNN的表现要稍微好于RNN的效果 Paper Link Transformer统治的时代，LSTM模型并没有被代替，LSTM比Tranformer优势在哪里？ - Segmentation的回答 - 知乎 Transformer统治的时代，LSTM模型并没有被代替，LSTM比Tranformer优势在哪里？ - DengBoCong的回答 - 知乎","categories":[{"name":"transformer","slug":"transformer","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/transformer/"}],"tags":[]},{"title":"Bert中的词向量各项异性","slug":"Bert中的词向量各项异性","date":"2022-11-16T01:43:20.000Z","updated":"2022-11-28T13:49:34.264Z","comments":true,"path":"2022/11/16/Bert中的词向量各项异性/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/16/Bert%E4%B8%AD%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E5%90%84%E9%A1%B9%E5%BC%82%E6%80%A7/","excerpt":"","text":"概念 各向异性(Anisotropic) 的概念在BERT-flow的文章中有明确的定义： “Anisotropic” means word embeddings occupy a narrow cone in the vector space. 翻译过来就是：“各向异性”表示词嵌入在向量空间中占据了一个狭窄的圆锥形体。 在向量空间中： 各项异性就是分布与分布方向有关系，而各项同性就是各个方向都一样，以二维空间为例： 部分学者发现，Transformer学到的词向量在空间的分布如下： Bert与GPT-2中也存在类似的情况。由上图可以知道模型学到的向量分布是各项异性的。 各项异性的缺点 各向异性就有个问题，那就是最后学到的向量都挤在一起，彼此之间计算余弦相似度都很高，并不是一个很好的表示。一个好的向量表示应该同时满足Alignment 和 uniformity，前者表示相似的向量距离应该相近，后者就表示向量在空间上应该尽量均匀，最好是各向同性的。 左图是理想的表示，右图则有各向异性的缺点。 应对方法 1. 映射为各向同性 BERT-flow的工作就是将原来的分布校准为高斯分布。标准的高斯分布就是各向同性的。 img 类似的还有whitening操作。大概流程就是根据SVD分解的结果，旋转缩放后得到一个标准正态分布。 2. 消除主成分 参见论文： A Simple but Tough-to-Beat Baseline for Sentence Embeddings All-but-the-Top: Simple and Effective Postprocessing for Word Representations 3. 正则化 参见论文： Representation Degeneration Problem in Training Natural Language Generation Models 本文选自： 作者：看图学 链接：https://www.zhihu.com/question/460991118/answer/2353153090 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"nlp","slug":"nlp","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/"},{"name":"bert","slug":"nlp/bert","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/bert/"}],"tags":[]},{"title":"Pandas-excel-数据追加","slug":"Pandas-excel-数据追加","date":"2022-11-13T09:14:00.000Z","updated":"2022-11-15T13:31:00.021Z","comments":true,"path":"2022/11/13/Pandas-excel-数据追加/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/13/Pandas-excel-%E6%95%B0%E6%8D%AE%E8%BF%BD%E5%8A%A0/","excerpt":"","text":"不同于 csv ，无法直接通过mode=\"a\"，进行追加。 具体方法: 读取要追加数据的excel文件,然后与要追加的数据组合一起保存到原文件中 12345678#--coding-- utf-8--import pandasdef append_to_excel(filepath,dataframe)-&gt;None: writer=pandas.ExcelWriter(filepath,mode=&#x27;w&#x27;)#这里的mode需要用w模式，a模式会产生新的sheet data=pandas.read_excel(writer,index_col=None,header=None) data.to_excel(writer,startrow=0,index=None,header=None,sheet_name=&#x27;sheet1&#x27;) dataframe.to_excel(writer,startrow=data.shape[0],index=None,header=None,sheet_name=&#x27;sheet1&#x27;) writer.save() 感谢： (43条消息) python pandas excel同一个sheet追加不覆盖追加数据_Demnok Lannik的博客-CSDN博客_python panada 追加sheet","categories":[{"name":"Pandas","slug":"Pandas","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pandas/"}],"tags":[]},{"title":"Pytorch-data-DataLoader使用","slug":"Pytorch-data-DataLoader使用","date":"2022-11-13T06:44:06.000Z","updated":"2023-02-10T07:34:56.115Z","comments":true,"path":"2022/11/13/Pytorch-data-DataLoader使用/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/13/Pytorch-data-DataLoader%E4%BD%BF%E7%94%A8/","excerpt":"","text":"简介 torch.utils.data.DataLoader主要是对数据进行batch的划分，除此之外，特别要注意的是输入进函数的数据一定得是可迭代的。如果是自定的数据集的话可以在定义类中用def__len__、def__getitem__定义。 使用DataLoader的好处是，可以快速的迭代数据。 示例 123456789101112131415161718192021222324252627282930313233343536373839import torchimport torch.utils.data as Datatorch.manual_seed(1) # reproducible BATCH_SIZE = 5 # 批训练的数据个数 x = torch.linspace(1, 10, 10) # x data (torch tensor)y = torch.linspace(10, 1, 10) # y data (torch tensor) # 先转换成 torch 能识别的 Datasettorch_dataset = Data.TensorDataset(x, y) # 把 dataset 放入 DataLoaderloader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # 要不要打乱数据 (打乱比较好) num_workers=2, # 多线程来读数据) for epoch in range(3): # 训练所有!整套!数据 3 次 for step, (batch_x, batch_y) in enumerate(loader): # 每一步 loader 释放一小批数据用来学习 # 假设这里就是你训练的地方... # 打出来一些数据 print(&quot;&#x27;Epoch: &#x27;&quot;, epoch, &quot;&#x27;| Step: &#x27;&quot;, step, &quot;&#x27;| batch x: &#x27;&quot;,batch_x.numpy(), \\ &quot;&#x27;| batch y: &#x27;&quot;, batch_y.numpy()) &quot;&quot;&quot;Epoch: 0 | Step: 0 | batch x: [ 6. 7. 2. 3. 1.] | batch y: [ 5. 4. 9. 8. 10.]Epoch: 0 | Step: 1 | batch x: [ 9. 10. 4. 8. 5.] | batch y: [ 2. 1. 7. 3. 6.]Epoch: 1 | Step: 0 | batch x: [ 3. 4. 2. 9. 10.] | batch y: [ 8. 7. 9. 2. 1.]Epoch: 1 | Step: 1 | batch x: [ 1. 7. 8. 5. 6.] | batch y: [ 10. 4. 3. 6. 5.]Epoch: 2 | Step: 0 | batch x: [ 3. 9. 2. 6. 7.] | batch y: [ 8. 2. 9. 5. 4.]Epoch: 2 | Step: 1 | batch x: [ 10. 4. 8. 1. 5.] | batch y: [ 1. 7. 3. 10. 6.]&quot;&quot;&quot;“”如果改变batch大小每次迭代数据不够batch,则函数就会把剩下的数据输出。“” PyTorch DataLoader工作原理可视化 (qq.com) (51条消息) pytorch中collate_fn函数的使用&amp;如何向collate_fn函数传参_XJTU-Qidong的博客-CSDN博客_pytorch collate_fn","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"对比学习","slug":"对比学习","date":"2022-11-09T01:23:17.000Z","updated":"2022-11-15T13:30:10.687Z","comments":true,"path":"2022/11/09/对比学习/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/09/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"对比学习属于无监督或者自监督学习 目前，对比学习貌似处于“无明确定义、有指导原则”的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。而如何构造相似实例，以及不相似实例，如何构造能够遵循上述指导原则的表示学习模型结构，以及如何防止模型坍塌(Model Collapse)，这几个点是其中的关键。 目前出现的一些方法： 从防止模型坍塌的不同方法角度，我们可大致把现有方法划分为：基于负例的对比学习方法、基于对比聚类的方法、基于不对称网络结构的方法，以及基于冗余消除损失函数的方法。","categories":[{"name":"contrastive","slug":"contrastive","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/contrastive/"}],"tags":[]},{"title":"Pytorch-nn-Embedding","slug":"Pytorch-nn-Embedding","date":"2022-11-04T02:51:22.000Z","updated":"2022-11-15T13:32:52.492Z","comments":true,"path":"2022/11/04/Pytorch-nn-Embedding/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/11/04/Pytorch-nn-Embedding/","excerpt":"","text":"简述 123torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None) 其为一个简单的存储固定大小的词典的嵌入向量的查找表，意思就是说，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。 输入为一个编号列表，输出为对应的符号嵌入向量列表。 参数说明 num_embeddings (python:int) – 词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999） embedding_dim (python:int)– 嵌入向量的维度，即用多少维来表示一个符号。 padding_idx (python:int, optional) – 填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0） max_norm (python:float, optional) – 最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化 norm_type (python:float, optional) – 指定利用什么范数计算，并用于对比max_norm，默认为2范数。 scale_grad_by_freq (boolean, optional) – 根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False. sparse (bool, optional) – 若为True,则与权重矩阵相关的梯度转变为稀疏张量。 实例 123456789101112131415161718192021222324&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3&gt;&gt;&gt; embedding = nn.Embedding(10, 3)&gt;&gt;&gt; # a batch of 2 samples of 4 indices each&gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])&gt;&gt;&gt; embedding(input)tensor([[[-0.0251, -1.6902, 0.7172], [-0.6431, 0.0748, 0.6969], [ 1.4970, 1.3448, -0.9685], [-0.3677, -2.7265, -0.1685]], [[ 1.4970, 1.3448, -0.9685], [ 0.4362, -0.4004, 0.9400], [-0.6431, 0.0748, 0.6969], [ 0.9124, -2.3616, 1.1151]]]) &gt;&gt;&gt; # example with padding_idx&gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)&gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]])&gt;&gt;&gt; embedding(input)tensor([[[ 0.0000, 0.0000, 0.0000], [ 0.1535, -2.0309, 0.9315], [ 0.0000, 0.0000, 0.0000], [-0.1655, 0.9897, 0.0635]]])","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"(Pytorch)中的一些基本方法","slug":"Pytorch-中的一些基本方法","date":"2022-10-28T03:18:51.000Z","updated":"2022-12-02T01:02:35.533Z","comments":true,"path":"2022/10/28/Pytorch-中的一些基本方法/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/28/Pytorch-%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95/","excerpt":"1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法 7. torch.split方法","text":"1. torch.chunk方法 2. torch.cumsum方法 3. torch.cat方法 4. torch.stack方法 5. torch.max方法 6. squeeze与unsqueezef方法 7. torch.split方法 1. torch.chunk方法 1Tensor.chunk(chunks,dim=0) # 可以参考torch.chunk() torch.chunk tensor (Tensor) – the tensor to split chunks (int) - number of chunks to return（分割的块数） dim (int) - dimension along which to split the tensor（沿着哪个轴分块） 实例： 实例 2. torch.cumsum方法 返回 输入张量指定维度累加和 1torch.cumsum(input, dim, *, dtype=None, out=None) → Tensor 参数： input (Tensor) 输入张量 dim (int) 操作的维度 计算原理: cumsum 计算原理 3. torch.cat方法 在给定维数中连接给定序列的 seq 张量。所有张量必须具有相同的形状(连接维数除外)或为空。 1torch.cat(tensors, dim=0, *, out=None) → Tensor 实例： 12345678910111213141516&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) 4. torch.stack方法 沿着一个新的维数连接一系列张量。 所有张量的大小必须相同。 1torch.stack(tensors, dim=0, *, out=None) → Tensor # 插入了一个维度 实例： 1234567891011import torchl = 4tag = torch.FloatTensor(l, l, 2).fill_(0)tag1 = torch.FloatTensor(l, l, 2).fill_(5)tag2 = torch.FloatTensor(l, l, 2).fill_(2)tags = torch.stack([tag,tag1,tag2],dim=0)display(tags.shape)tags = torch.stack([tag,tag1,tag2],dim=1)tags.shape output: 12torch.Size([3, 4, 4, 2])torch.Size([4, 3, 4, 2]) 5. torch.max方法 方式一： 1torch.max(input) → Tensor 返回所有元素的最大值 实例： 12345&gt;&gt;&gt; a = torch.randn(1, 3)&gt;&gt;&gt; atensor([[ 0.6763, 0.7445, -2.2369]])&gt;&gt;&gt; torch.max(a)tensor(0.7445) 方式二： 1torch.max(input, dim, keepdim=False, *, out=None) 返回一个命名元组(values, indexes)，其中values是给定维度dim中输入张量的每一行的最大值。而indexes是找到的每个最大值的索引位置(argmax)。 实例： 12345678&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]])&gt;&gt;&gt; torch.max(a, 1)torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1])) 方式二，个人理解： ​ 给定一个张量 t 和维度d，设 t 的维度为 x*y*z 最后的返回值values（仅表示思路 ）: 12345d = 0 # 举例output = torch.zeros(y,z)for i in range(y): for j in range(z): output[i][j] = max(t[:][i][j]) 6. squeeze与unsqueezef方法 给tensor删除或者添加维度为1的维度 squeeze() 方法 在pytorch中，用torch.squeeze()函数或者tensor的自身成员函数squeeze()去除维度为1的维度。 使用示例: 12345678x = torch.randn(3,1,2,4,1)print(x.shape)x_ = x.squeeze() # 默认删除所大小为1 的维度print(x_.shape)y = torch.squeeze(x,dim=1) # 指定维度print(y.shape)z = torch.squeeze(x,dim=2) # dim=2 维度不是1 无法去除 但不会报错print(z.shape) 1234torch.Size([3, 1, 2, 4, 1])torch.Size([3, 2, 4])torch.Size([3, 2, 4, 1])torch.Size([3, 1, 2, 4, 1]) unsqueeze() 方法 在pytorch中，用自带的torch.unsqueeze()和tensor的成员函数unsqueeze()可以为tensor添加维度为1的维度 1234e = torch.unsqueeze(x, dim=0) # 在第一维度添加维度print(e.shape)f = x.unsqueeze(dim=0)print(f.shape) 12torch.Size([1, 3, 1, 4, 1, 2])torch.Size([1, 3, 1, 4, 1, 2]) 7. torch.split方法 按块大小拆分张量 1torch.split(tensor, split_size_or_sections, dim = 0) tensor 为待拆分张量 dim 指定张量拆分的所在维度，即在第几维对张量进行拆分 split_size_or_sections 表示在 dim 维度拆分张量时每一块在该维度的尺寸大小 (int)，或各块尺寸大小的列表 (list) 指定每一块的尺寸大小后，如果在该维度无法整除，则最后一块会取余数，尺寸较小一些 如：长度为 10 的张量，按单位长度 3 拆分，则前三块长度为 3，最后一块长度为 1 函数返回：所有拆分后的张量所组成的 tuple 函数并不会改变原 tensor 实例： 12345a = torch.arange(10).reshape(5,2)b,c = torch.split(a,1,dim=-1)print(a)print(b.squeeze())print(c.squeeze()) 1234567tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])tensor([0, 2, 4, 6, 8])tensor([1, 3, 5, 7, 9])","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"(Pytorch) nn.Dropout","slug":"Pytorch-nn-Dropout","date":"2022-10-27T09:04:16.000Z","updated":"2022-11-15T13:32:14.476Z","comments":true,"path":"2022/10/27/Pytorch-nn-Dropout/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/27/Pytorch-nn-Dropout/","excerpt":"","text":"目的： 为了防止过拟合 Dropout防止过拟合 具体用法 1nn.Dropout(0.2) #表示每个输入的神经元有 0.2的概率被设为0 从下图中也可以看到，其他的数 除了（1-0.3） 补充说明: Dropout 只能用于训练部分，不可用于测试 一般用在全连接神经网络映射层之后 还可用于将Tensor中的部分值设为0 如上图","categories":[{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"}],"tags":[]},{"title":"my first blog","slug":"my-first-blog","date":"2022-10-24T01:14:45.000Z","updated":"2022-11-20T01:28:36.846Z","comments":true,"path":"2022/10/24/my-first-blog/","link":"","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/2022/10/24/my-first-blog/","excerpt":"","text":"我的第一个博客 你好世界！ 博客模板： fi3ework/hexo-theme-archer: 🎯 A smart and modern theme for Hexo. (github.com) memaid测试 graph LR 1(CIR-Lab) --> 1.1(实验室共享文件) -->小组A学习资料 1.1--> 小组B学习资料 1.1--> 研究生课程学习与考试资料等 1.1--> 文件互传 style 1.1 fill:#0f0,stroke:#333,stroke-width:4px 1 --> 1.2(实验室项目资料) -->项目A资料 style 1.2 fill:#f91,stroke:#333,stroke-width:4px 1.2--> 软著资料 1.2--> 专利资料 1.2--> 论文资料 latex公式测试 \\[ \\begin{equation} \\left\\{ \\begin{array}{lr} x=\\dfrac{3\\pi}{2}(1+2t)\\cos(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\\\ y=s, &amp; 0 \\leq s \\leq L,|t| \\leq1. \\\\ z=\\dfrac{3\\pi}{2}(1+2t)\\sin(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\end{array} \\right. \\end{equation} \\]","categories":[],"tags":[]}],"categories":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Tensorflow/"},{"name":"Pytorch","slug":"Pytorch","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pytorch/"},{"name":"深度学习基础","slug":"深度学习基础","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"},{"name":"Python","slug":"Python","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Python/"},{"name":"transformer","slug":"transformer","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/transformer/"},{"name":"nlp","slug":"nlp","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/"},{"name":"bert","slug":"nlp/bert","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/nlp/bert/"},{"name":"Pandas","slug":"Pandas","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/Pandas/"},{"name":"contrastive","slug":"contrastive","permalink":"https://github.com/vacuum-cup/vacuum-cup.github.io/categories/contrastive/"}],"tags":[]}